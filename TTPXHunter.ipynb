{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JM-tfdiUqKIClHdfY4CKjGSludNwVv-B","authorship_tag":"ABX9TyPhYOjCnGt608EQ+YfW9knU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","import pickle\n","import torch\n","import os"],"metadata":{"id":"cO_mCsQqov8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srVGFyN6MEG1","executionInfo":{"status":"ok","timestamp":1726927937708,"user_tz":-330,"elapsed":3687,"user":{"displayName":"Review Paper","userId":"09140924793698071071"}},"outputId":"c7ac8731-47b1-40e7-938b-86f85dcb129f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Load TTPXHunter from Huggingface"],"metadata":{"id":"eg-b6DVSn_Z2"}},{"cell_type":"code","source":["# Load the model and tokenizer from the Hugging Face Hub\n","model = RobertaForSequenceClassification.from_pretrained(\"nanda-rani/TTPXHunter\")\n","tokenizer = RobertaTokenizer.from_pretrained(\"nanda-rani/TTPXHunter\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# Copy the model to the GPU.\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kf7xE9b2nsh5","executionInfo":{"status":"ok","timestamp":1726927941169,"user_tz":-330,"elapsed":3463,"user":{"displayName":"Review Paper","userId":"09140924793698071071"}},"outputId":"82d15ebf-1433-4bf3-974f-6180b3e79b42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=193, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def extract_ttp_from_sentences(sentences, threshold, label_dict, ttpid2name):\n","    \"\"\"\n","    Extract TTP (Tactics, Techniques, and Procedures) based on a prediction threshold from the given sentences.\n","\n","    Args:\n","    - sentences (list of str): List of sentences to extract TTP from.\n","    - threshold (float): Confidence threshold for accepting predictions.\n","\n","    Returns:\n","    - unique_ttp_ids (list of int): Unique TTP IDs extracted from the sentences.\n","    - names_for_ttp_ids (list of str): Human-readable names corresponding to the TTP IDs.\n","    \"\"\"\n","    predictions = []\n","\n","    # Loop over sentences and perform inference\n","    for text in sentences:\n","        # Tokenize the input text\n","        inputs = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n","\n","        # Perform inference without gradient tracking\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        # Extract logits and compute probabilities\n","        logits = outputs.logits\n","        probabilities = torch.softmax(logits, dim=1)\n","        max_prob, predicted_class_indices = torch.max(probabilities, dim=1)\n","\n","        # Filter predictions based on the confidence threshold\n","        predicted_labels = [\n","            model.config.id2label[class_idx.item()]\n","            for prob, class_idx in zip(max_prob, predicted_class_indices)\n","            if prob.item() > threshold\n","        ]\n","\n","        predictions.extend(predicted_labels)\n","\n","    # Map the predicted labels to integer labels\n","    mapped_labels = [int(label.split('_')[1]) for label in predictions]\n","\n","    # Load the label-to-name dictionary\n","    with open(label_dict, 'rb') as file:\n","        label_dict = pickle.load(file)\n","\n","    # Invert the dictionary to map integer labels to TTP names\n","    inverted_label_dict = {v: k for k, v in label_dict.items()}\n","    ttp_list = [inverted_label_dict[label] for label in mapped_labels]\n","\n","    # Extract unique TTP IDs\n","    unique_ttp_ids = list(set(ttp_list))\n","\n","    # Translate TTP IDs to their names\n","    names_for_ttp_ids = translate_ttp_ids_to_names(unique_ttp_ids, ttpid2name)\n","\n","    return unique_ttp_ids, names_for_ttp_ids\n","\n","def remove_consecutive_newlines(text):\n","    \"\"\"\n","    Remove consecutive newlines from a string.\n","\n","    Args:\n","    - text (str): Input string with potential consecutive newlines.\n","\n","    Returns:\n","    - str: String with consecutive newlines reduced to single newlines.\n","    \"\"\"\n","    cleaned_text = text[0]\n","    for char in text[1:]:\n","        if not (char == cleaned_text[-1] and cleaned_text[-1] == '\\n'):\n","            cleaned_text += char\n","    return cleaned_text\n","\n","def process_text_file_for_attack_patterns(file_name, threshold, label_dict, ttpid2name):\n","    \"\"\"\n","    Read and process a text file to extract attack patterns using TTP extraction.\n","\n","    Args:\n","    - file_name (str): Path to the input text file.\n","    - threshold (float): Confidence threshold for TTP extraction.\n","\n","    Returns:\n","    - tuple: (unique TTP IDs, names corresponding to TTP IDs).\n","    \"\"\"\n","    sentences = []\n","\n","    # Read the text file\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Clean the text by removing consecutive newlines and tabs\n","    text = remove_consecutive_newlines(text)\n","    text = text.replace('\\t', ' ').replace(\"\\'\", \"'\")\n","\n","    # Tokenize sentences\n","    tokenized_sentences = nltk.sent_tokenize(text)\n","\n","    # Split tokenized sentences by newlines and filter empty lines\n","    for sentence in tokenized_sentences:\n","        sentences += [line for line in sentence.split('\\n') if len(line) > 0]\n","\n","    # Extract TTP from the processed sentences\n","    return extract_ttp_from_sentences(sentences, threshold, label_dict, ttpid2name)\n","\n","def translate_ttp_ids_to_names(ttp_ids, ttpid2name):\n","    \"\"\"\n","    Translate TTP (Tactics, Techniques, and Procedures) IDs to human-readable names.\n","\n","    Args:\n","    - ttp_ids (list of int): List of TTP IDs to translate.\n","\n","    Returns:\n","    - list of str: Corresponding human-readable names for the TTP IDs.\n","    \"\"\"\n","    # Load the TTP ID to name mapping from a file\n","    with open(ttpid2name, 'rb') as file:\n","        id_to_name_map = pickle.load(file)\n","\n","    # Translate each TTP ID to its corresponding name\n","    ttp_names = [id_to_name_map[ttp_id] for ttp_id in ttp_ids if ttp_id in id_to_name_map]\n","\n","    return ttp_names\n"],"metadata":{"id":"SFX5RzAQ5B1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = 'label_dict.pkl'\n","ttpid2name = 'ttp_id_name.pkl'\n","report = \"SharpPanda_APT_Campaign_Expands_its_Arsenal_Targeting_G20_Nations.txt\"\n","th = 0.644\n","\n","ttps, ttp_names = process_text_file_for_attack_patterns(report, th, label_dict, ttpid2name)\n","print(len(ttps))\n","\n","for i in range(len(ttps)):\n","  print(ttps[i], \" - \", ttp_names[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuDw4osy5RHL","executionInfo":{"status":"ok","timestamp":1726927962479,"user_tz":-330,"elapsed":21312,"user":{"displayName":"Review Paper","userId":"09140924793698071071"}},"outputId":"65d9b483-3086-495c-d011-4e1cc224be75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["19\n","T1588  -  Obtain Capabilities\n","T1531  -  Account Access Removal\n","T1566  -  Phishing\n","T1203  -  Exploitation for Client Execution\n","T1082  -  System Information Discovery\n","T1140  -  Deobfuscate/Decode Files or Information\n","T1560  -  Archive Collected Data\n","T1105  -  Ingress Tool Transfer\n","T1480  -  Execution Guardrails\n","T1218  -  System Binary Proxy Execution\n","T1036  -  Masquerading\n","T1210  -  Exploitation of Remote Services\n","T1119  -  Automated Collection\n","T1027  -  Obfuscated Files or Information\n","T1041  -  Exfiltration Over C2 Channel\n","T1568  -  Dynamic Resolution\n","T1205  -  Traffic Signaling\n","T1587  -  Develop Capabilities\n","T1005  -  Data from Local System\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zc1q-gWv7HKJ"},"execution_count":null,"outputs":[]}]}